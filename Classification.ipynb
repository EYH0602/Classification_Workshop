{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcpTPPcqP8ca"
      },
      "source": [
        "# Classification Algorithms\n",
        "\n",
        "As its name tells, these algorithm put items into classes by the features of the items. \n",
        "These classes could be binary (yes or no) or no. For example, predicting hand-written digits into 10 classes (0-9).\n",
        "\n",
        "This workshop we will take a look a three of the traditional ML algo on classifications with no neuron network involved. We will work on the [mlb dataset](https://github.com/matloff/regtools/blob/master/data/mlb.txt.gz).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL86E1U6THix"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d86usk_GTU0u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start by defining a class for general classification model.\n",
        "\n",
        "This kind of model holds data, and provides interfaces for `train`, `test`, and `predict`.\n",
        "\n",
        "For this workshop, since we are doing classification, we will measure the performance by the fraction of correct guess, which is `accuracy_score` provided in `sklearn` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    \n",
        "    def train(self):\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def predict_group(self, X):\n",
        "        y_pred = [self.predict(x) for x in X]\n",
        "        return y_pred\n",
        "\n",
        "    def test(self):\n",
        "        predictions = self.predict_group(self.data[\"X_test\"])\n",
        "        return accuracy_score(self.data[\"y_test\"], predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeydEe6gTPpY"
      },
      "source": [
        "\n",
        "### Data Pre-processing\n",
        "\n",
        "We will start by downloading and reshaping the data.\n",
        "[mlb](https://github.com/matloff/regtools/blob/master/data/mlb.txt.gz) dataset contains Major Baseball League player info\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Team</th>\n",
              "      <th>Position</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Age</th>\n",
              "      <th>PosCategory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adam_Donachie</td>\n",
              "      <td>BAL</td>\n",
              "      <td>Catcher</td>\n",
              "      <td>74</td>\n",
              "      <td>180</td>\n",
              "      <td>22.99</td>\n",
              "      <td>Catcher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Paul_Bako</td>\n",
              "      <td>BAL</td>\n",
              "      <td>Catcher</td>\n",
              "      <td>74</td>\n",
              "      <td>215</td>\n",
              "      <td>34.69</td>\n",
              "      <td>Catcher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ramon_Hernandez</td>\n",
              "      <td>BAL</td>\n",
              "      <td>Catcher</td>\n",
              "      <td>72</td>\n",
              "      <td>210</td>\n",
              "      <td>30.78</td>\n",
              "      <td>Catcher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Kevin_Millar</td>\n",
              "      <td>BAL</td>\n",
              "      <td>First_Baseman</td>\n",
              "      <td>72</td>\n",
              "      <td>210</td>\n",
              "      <td>35.43</td>\n",
              "      <td>Infielder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Chris_Gomez</td>\n",
              "      <td>BAL</td>\n",
              "      <td>First_Baseman</td>\n",
              "      <td>73</td>\n",
              "      <td>188</td>\n",
              "      <td>35.71</td>\n",
              "      <td>Infielder</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Name Team       Position  Height  Weight    Age PosCategory\n",
              "1    Adam_Donachie  BAL        Catcher      74     180  22.99     Catcher\n",
              "2        Paul_Bako  BAL        Catcher      74     215  34.69     Catcher\n",
              "3  Ramon_Hernandez  BAL        Catcher      72     210  30.78     Catcher\n",
              "4     Kevin_Millar  BAL  First_Baseman      72     210  35.43   Infielder\n",
              "5      Chris_Gomez  BAL  First_Baseman      73     188  35.71   Infielder"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_df = pd.read_csv(\"mlb.txt\", sep=\" \")\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def splitData(df, train_frac=0.8):\n",
        "    nrow = df.shape[0]\n",
        "    train_len = math.floor(nrow * train_frac)\n",
        "    test_len = nrow - train_len\n",
        "    return df.head(train_len), df.tail(test_len)\n",
        "\n",
        "train, test = splitData(raw_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = train[['Height', 'Age', 'Weight']]\n",
        "y_train = train['Position'].map(lambda x: int(x == 'Catcher'))\n",
        "X_test = test[['Height', 'Age', 'Weight']]\n",
        "y_test = test['Position'].map(lambda x: int(x == 'Catcher'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhpK644rSDBB"
      },
      "source": [
        "## K Nearest Neighbors (KNN)\n",
        "\n",
        "In this section, we will use kNN algorithm to predict a particular player is or is not a Catcher from the user's identities (Weight, Height, Age).\n",
        "\n",
        "The key idea of kNN is to find the distance of a instance to the existing data, then use the closest one as the guess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class kNN:\n",
        "    def __init__(self,k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X.to_numpy()\n",
        "        self.y_train = y.to_numpy()\n",
        "\n",
        "        \n",
        "    def predict_df(self, X):\n",
        "        predicted_labels = [self._predict(x) for x in X.to_numpy()]\n",
        "        return np.array(predicted_labels)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self._predict(np.array(X))\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # compute the distances\n",
        "        distances = [self._distance(x, x_train) for x_train in self.X_train]\n",
        "        \n",
        "        # get the k-nearest samples, labels\n",
        "        # sort distances, return an array of indices, get only first k\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        # majority vote, most common class label\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)\n",
        "        \n",
        "        return most_common[0][0]\n",
        "    \n",
        "    def _distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = kNN()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "model.predict([100, 23, 200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of our model:  0.9310344827586207\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict_df(X_test)\n",
        "print(\n",
        "    \"The accuracy of our model: \", \n",
        "    accuracy_score(y_test, predictions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOi-sUXYjth1"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "The idea of this algorithm comes directly from conditional probability (Bayes Theorem).\n",
        "$$\n",
        "P(y | X) = \\frac{P(X | y) \\cdot P(y)}{P(X)} = \\frac{P(X_1 | y) \\cdots P(X_n | y) \\cdot P(y)}{P(X)}\n",
        "$$\n",
        "assuming all features are independent.\n",
        "\n",
        "So if we want to choose the $y$ with highest probability, it would be\n",
        "$$\n",
        "y = \\argmax_y P(y | X) = \\argmax_y \\frac{P(X_1 | y) \\cdots P(X_n | y) \\cdot P(y)}{P(X)}\n",
        "$$\n",
        "\n",
        "For each $P(X_i | y)$ assume normal distribution, the probability is\n",
        "$$\n",
        "P(X_i | y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{- \\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\n",
        "$$\n",
        "where $\\sigma$ is the standard deviation of $y$ and $\\mu$ is the mean of $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qN9w1-Afj3aY"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self._classes = []\n",
        "        self._mean = {}\n",
        "        self._var = {}\n",
        "        self._priors = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # find unique elements of y, use as classes\n",
        "        self._classes = np.unique(y)\n",
        "\n",
        "        for c in self._classes:    # for each class\n",
        "            X_c = X[y == c]\n",
        "            self._mean[c] = X_c.mean(axis=0)    # find mean of each column\n",
        "            self._var[c] = X_c.var(axis=0)\n",
        "            # frequency of this class in training sample\n",
        "            self._priors[c] = X_c.shape[0]/float(n_samples)\n",
        "\n",
        "    def predict_df(self, X):\n",
        "        X = X.to_numpy()\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return y_pred\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return self._predict(np.array(X))\n",
        "\n",
        "    def _predict(self, x):    # one sample\n",
        "        posteriors = []\n",
        "        for c in self._classes:\n",
        "            prior = self._priors[c]    # with current index\n",
        "            class_conditional = np.sum(np.log(self._pdf(c, x)))\n",
        "            posterior = prior + class_conditional\n",
        "            posteriors.append(posterior)\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "\n",
        "    def _pdf(self, class_name, x):    # probability density function\n",
        "        mean = self._mean[class_name].values\n",
        "        var = self._var[class_name].values\n",
        "\n",
        "        numerator = np.exp(- (x-mean)**2/(2*var))\n",
        "        denominator = np.sqrt(2*np.pi*var)\n",
        "        return numerator/denominator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NaiveBayes()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "model.predict([100, 23, 200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy of our model:  0.896551724137931\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict_df(X_test)\n",
        "print(\n",
        "    \"The accuracy of our model: \", \n",
        "    accuracy_score(y_test, predictions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkCmjn_ZjwnZ"
      },
      "source": [
        "## Support Vector Machine (SVM)\n",
        "\n",
        "Here is the part where we really introduce how machine \"learns\".\n",
        "KNN and naive bayes are just making predictions based on the relationship between old data (train data) and new data (test data).\n",
        "Whereas in SVM, the algorithm finds and memorizes the parameters of a \"plane\" separating the data, and the classify the new data based on this memorized parameters (plane).\n",
        "\n",
        "Therefore, the problem becomes to learn the parameters that determines the separating hyperplane.\n",
        "This hyperplane should be as far from the data points (support vector) as possible.\n",
        "In other words, the margin between hyperplane and support vectors should be as large as possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "We can define the hyperplane by\n",
        "$$\n",
        "\\mathbf{w}^{T}\\mathbf{x} + b\n",
        "$$\n",
        "where $\\mathbf{w}$ is the vector describing the plane, $\\mathbf{x}$ is any given points, and $b$ is a constant.\n",
        "In this case, $\\mathbf{w}$ is what we need to find out.\n",
        "\n",
        "The distance between any given point $\\mathbf{A}$ to the plane can be calculated by\n",
        "$$\n",
        "\\frac{|\\mathbf{w}^{T}\\mathbf{A} + b|}{||\\mathbf{w}||}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To maximize the margin, we can use a gradient based algorithm on the loss function.\n",
        "\n",
        "**NOTE** that we are trying to maximize something instead of minimize, so it would be a **gradient boost** algorithm instead of gradient decent.\n",
        "\n",
        "To do this, we need a loss function to represent the update\n",
        "$$\n",
        "l(x, y, w) = \\min_{w} \\lambda ||w||^2 + \\sum_{i = 1}^{n} (1 - y_i \\langle x_i, w \\rangle)_{+}\n",
        "$$\n",
        "where $\\langle \\cdot, \\cdot \\rangle$ represent distance and subscript $\\cdot_{+}$ represent only when the value is positive, else 0.\n",
        "\n",
        "With the loss function defined, we can find the minimum by calculating the gradient\n",
        "$$\n",
        "\\sigma = \\frac{\\partial l}{\\partial w_k} = 2 \\lambda w_k - (y_i x_{ik})_+\n",
        "$$\n",
        "\n",
        "Thus, we can update the weights by\n",
        "\n",
        "$$\n",
        "w = w + \\alpha \\sigma\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9359605911330049\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "l = len(X_train)\n",
        "train_f1 = np.array(X_train.iloc[:, 0])\n",
        "train_f2 = np.array(X_train.iloc[:, 1])\n",
        "w1 = np.zeros((l, 1))\n",
        "w2 = np.zeros((l, 1))\n",
        "\n",
        "train_f1 = np.reshape(train_f1, (l, 1))\n",
        "train_f2 = np.reshape(train_f2, (l, 1))\n",
        "y_train = list(map(lambda x: -1 if x == 0 else x, y_train))\n",
        "y_train = np.array(y_train)\n",
        "y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "\n",
        "epochs = 1\n",
        "alpha = 0.0001\n",
        "\n",
        "while(epochs < 1000):\n",
        "    y = w1 * train_f1  + w2 * train_f2\n",
        "    prod = y * y_train\n",
        "    count = 0\n",
        "    for val in prod:\n",
        "        if(val >= 1):\n",
        "            cost = 0\n",
        "            w1 = w1 - alpha * (2 * 1/epochs * w1)\n",
        "            w2 = w2 - alpha * (2 * 1/epochs * w2)\n",
        "            \n",
        "        else:\n",
        "            cost = 1 - val \n",
        "            w1 = w1 + alpha * (train_f1[count] * y_train[count] - 2 * 1/epochs * w1)\n",
        "            w2 = w2 + alpha * (train_f2[count] * y_train[count] - 2 * 1/epochs * w2)\n",
        "        count += 1\n",
        "    epochs += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9359605911330049\n"
          ]
        }
      ],
      "source": [
        "\n",
        "l_test = len(X_test)\n",
        "index = list(range(l_test, l))\n",
        "w1 = np.delete(w1, index)\n",
        "w2 = np.delete(w2, index)\n",
        "w1 = w1.reshape(l_test, 1)\n",
        "w2 = w2.reshape(l_test, 1)\n",
        "\n",
        "\n",
        "## Extract the test data features \n",
        "test_f1 = np.array(X_test.iloc[:,0])\n",
        "test_f2 = np.array(X_test.iloc[:,1])\n",
        "test_f1 = test_f1.reshape(l_test, 1)\n",
        "test_f2 = test_f2.reshape(l_test, 1)\n",
        "\n",
        "\n",
        "# Predict\n",
        "y_pred = w1 * test_f1 + w2 * test_f2\n",
        "predictions = np.array(list(map(lambda val: int(val > 1), y_pred)))\n",
        "print(accuracy_score(y_test,predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HackerHub_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
